{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96426501",
   "metadata": {
    "papermill": {
     "duration": 0.002123,
     "end_time": "2025-10-17T15:56:25.747634",
     "exception": false,
     "start_time": "2025-10-17T15:56:25.745511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5586b0b7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-17T15:56:25.752239Z",
     "iopub.status.busy": "2025-10-17T15:56:25.751956Z",
     "iopub.status.idle": "2025-10-17T15:58:01.402264Z",
     "shell.execute_reply": "2025-10-17T15:58:01.401209Z"
    },
    "papermill": {
     "duration": 95.654431,
     "end_time": "2025-10-17T15:58:01.403997",
     "exception": false,
     "start_time": "2025-10-17T15:56:25.749566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'MedCLIP'...\r\n",
      "remote: Enumerating objects: 39, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (36/36), done.\u001b[K\r\n",
      "remote: Total 39 (delta 3), reused 23 (delta 2), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (39/39), 109.56 KiB | 6.44 MiB/s, done.\r\n",
      "Resolving deltas: 100% (3/3), done.\r\n",
      "/kaggle/working/MedCLIP\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.5/90.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "kaggle-environments 1.18.0 requires transformers>=4.33.1, but you have transformers 4.24.0 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.24.0 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m188b4993597842cf020e1c66ee7c75a7cb6e6ec5\r\n"
     ]
    }
   ],
   "source": [
    "# --- Kaggle Bootstrap Script ---\n",
    "%cd /kaggle/working\n",
    "%rm -rf MedCLIP\n",
    "!git clone --depth 1 https://github.com/lamlethanh777/MedCLIP.git\n",
    "\n",
    "%pwd\n",
    "%cd MedCLIP\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt --quiet\n",
    "\n",
    "# # Show current commit for reproducibility\n",
    "!git rev-parse HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b781c",
   "metadata": {
    "papermill": {
     "duration": 0.025135,
     "end_time": "2025-10-17T15:58:01.455252",
     "exception": false,
     "start_time": "2025-10-17T15:58:01.430117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pulling new changes from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95e4659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:58:01.506937Z",
     "iopub.status.busy": "2025-10-17T15:58:01.506666Z",
     "iopub.status.idle": "2025-10-17T15:58:01.916251Z",
     "shell.execute_reply": "2025-10-17T15:58:01.915227Z"
    },
    "papermill": {
     "duration": 0.437225,
     "end_time": "2025-10-17T15:58:01.917725",
     "exception": false,
     "start_time": "2025-10-17T15:58:01.480500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917070e2",
   "metadata": {
    "papermill": {
     "duration": 0.024645,
     "end_time": "2025-10-17T15:58:01.967860",
     "exception": false,
     "start_time": "2025-10-17T15:58:01.943215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convert Open-I dataset to compatible form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab03d695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:58:02.018730Z",
     "iopub.status.busy": "2025-10-17T15:58:02.018466Z",
     "iopub.status.idle": "2025-10-17T15:58:15.911436Z",
     "shell.execute_reply": "2025-10-17T15:58:15.910639Z"
    },
    "papermill": {
     "duration": 13.92037,
     "end_time": "2025-10-17T15:58:15.912997",
     "exception": false,
     "start_time": "2025-10-17T15:58:01.992627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Indiana dataset...\r\n",
      "Loaded 7466 projections\r\n",
      "Loaded 3851 reports\r\n",
      "Filtered to 3818 frontal views\r\n",
      "Merging projections and reports...\r\n",
      "Merged dataset size: 3818\r\n",
      "Found 3818 images that exist on disk\r\n",
      "After filtering reports: 3794 samples\r\n",
      "Extracting labels...\r\n",
      "Splitting data with validation ratio: 0.2\r\n",
      "Train set: 3036 samples\r\n",
      "Validation set: 758 samples\r\n",
      "\r\n",
      "Training Set Label Distribution:\r\n",
      "--------------------------------------------------\r\n",
      "No Finding                    : 2252 (74.18%)\r\n",
      "Enlarged Cardiomediastinum    :    0 ( 0.00%)\r\n",
      "Cardiomegaly                  :  263 ( 8.66%)\r\n",
      "Lung Lesion                   :    0 ( 0.00%)\r\n",
      "Lung Opacity                  :  351 (11.56%)\r\n",
      "Edema                         :   35 ( 1.15%)\r\n",
      "Consolidation                 :   23 ( 0.76%)\r\n",
      "Pneumonia                     :   33 ( 1.09%)\r\n",
      "Atelectasis                   :  257 ( 8.47%)\r\n",
      "Pneumothorax                  :   26 ( 0.86%)\r\n",
      "Pleural Effusion              :  117 ( 3.85%)\r\n",
      "Pleural Other                 :    0 ( 0.00%)\r\n",
      "Fracture                      :   77 ( 2.54%)\r\n",
      "Support Devices               :    0 ( 0.00%)\r\n",
      "--------------------------------------------------\r\n",
      "\r\n",
      "Validation Set Label Distribution:\r\n",
      "--------------------------------------------------\r\n",
      "No Finding                    :  587 (77.44%)\r\n",
      "Enlarged Cardiomediastinum    :    0 ( 0.00%)\r\n",
      "Cardiomegaly                  :   68 ( 8.97%)\r\n",
      "Lung Lesion                   :    0 ( 0.00%)\r\n",
      "Lung Opacity                  :   76 (10.03%)\r\n",
      "Edema                         :   10 ( 1.32%)\r\n",
      "Consolidation                 :    5 ( 0.66%)\r\n",
      "Pneumonia                     :    6 ( 0.79%)\r\n",
      "Atelectasis                   :   51 ( 6.73%)\r\n",
      "Pneumothorax                  :    2 ( 0.26%)\r\n",
      "Pleural Effusion              :   27 ( 3.56%)\r\n",
      "Pleural Other                 :    0 ( 0.00%)\r\n",
      "Fracture                      :   11 ( 1.45%)\r\n",
      "Support Devices               :    0 ( 0.00%)\r\n",
      "--------------------------------------------------\r\n",
      "\r\n",
      "✓ Saved training data to: ./local_data/indiana-train-meta.csv\r\n",
      "✓ Saved validation data to: ./local_data/indiana-val-meta.csv\r\n",
      "\r\n",
      "Preprocessing complete!\r\n",
      "\r\n",
      "You can now use 'indiana-train' and 'indiana-val' in your MedCLIP datalist.\r\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_indiana.py --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e1644",
   "metadata": {
    "papermill": {
     "duration": 0.025111,
     "end_time": "2025-10-17T15:58:15.963880",
     "exception": false,
     "start_time": "2025-10-17T15:58:15.938769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training MedCLIP with Open-I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea4a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T15:58:16.015264Z",
     "iopub.status.busy": "2025-10-17T15:58:16.014995Z",
     "iopub.status.idle": "2025-10-17T16:39:51.652660Z",
     "shell.execute_reply": "2025-10-17T16:39:51.651641Z"
    },
    "papermill": {
     "duration": 2495.665301,
     "end_time": "2025-10-17T16:39:51.654242",
     "exception": false,
     "start_time": "2025-10-17T15:58:15.988941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python examples/run_indiana_pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182050a",
   "metadata": {},
   "source": [
    "## Prompt Classification Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb2c33",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medclip import MedCLIPModel, MedCLIPVisionModelViT\n",
    "from medclip import MedCLIPProcessor\n",
    "from medclip import PromptClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334bb7f",
   "metadata": {},
   "source": [
    "### Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init models\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT, checkpoint='./data/MedCLIP/checkpoints/vision_text_pretrain/25000')\n",
    "clf = PromptClassifier(model, ensemble=True)\n",
    "clf.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30239022",
   "metadata": {},
   "source": [
    "### Prepare input image and prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input image\n",
    "from PIL import Image\n",
    "image = Image.open('./example_data/view1_frontal.jpg')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# prepare input prompt texts\n",
    "from medclip.prompts import generate_chexpert_class_prompts, process_class_prompts\n",
    "\n",
    "cls_prompts = process_class_prompts(generate_chexpert_class_prompts(n=10))\n",
    "inputs['prompt_inputs'] = cls_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9220e",
   "metadata": {},
   "source": [
    "### Run classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = clf(**inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75088a",
   "metadata": {},
   "source": [
    "## Model Evaluation on CheXpert Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b81374",
   "metadata": {},
   "source": [
    "### Load and prepare CheXpert validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c544063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to CheXpert validation set\n",
    "chexpert_root = Path('/kaggle/input/chexpert-v10-small')  # Update this path as needed\n",
    "valid_csv = chexpert_root / 'archive' / 'valid.csv'\n",
    "\n",
    "# Load validation data\n",
    "df_valid = pd.read_csv(valid_csv)\n",
    "\n",
    "# Select only frontal images and limit to ~100 samples\n",
    "df_valid_frontal = df_valid[df_valid['Frontal/Lateral'] == 'Frontal'].head(100)\n",
    "\n",
    "print(f\"Total validation samples: {len(df_valid)}\")\n",
    "print(f\"Selected frontal samples: {len(df_valid_frontal)}\")\n",
    "print(f\"\\nFirst few samples:\")\n",
    "print(df_valid_frontal.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf41ea",
   "metadata": {},
   "source": [
    "### Prepare CheXpert class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c410f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CheXpert class names (excluding 'No Finding' and 'Support Devices' for clinical evaluation)\n",
    "chexpert_classes = [\n",
    "    'Enlarged Cardiomediastinum',\n",
    "    'Cardiomegaly',\n",
    "    'Lung Opacity',\n",
    "    'Lung Lesion',\n",
    "    'Edema',\n",
    "    'Consolidation',\n",
    "    'Pneumonia',\n",
    "    'Atelectasis',\n",
    "    'Pneumothorax',\n",
    "    'Pleural Effusion',\n",
    "    'Pleural Other',\n",
    "    'Fracture'\n",
    "]\n",
    "\n",
    "print(f\"Evaluating on {len(chexpert_classes)} clinical classes:\")\n",
    "for i, cls in enumerate(chexpert_classes, 1):\n",
    "    print(f\"{i}. {cls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ca32c",
   "metadata": {},
   "source": [
    "### Run zero-shot classification on validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Prepare prompts for all CheXpert classes\n",
    "from medclip.prompts import generate_chexpert_class_prompts, process_class_prompts\n",
    "\n",
    "# Generate prompts for classification\n",
    "cls_prompts = process_class_prompts(generate_chexpert_class_prompts(n=10))\n",
    "\n",
    "# Store predictions and ground truth\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "valid_samples = 0\n",
    "\n",
    "print(\"Running zero-shot classification on validation samples...\")\n",
    "print(f\"Processing {len(df_valid_frontal)} images...\\n\")\n",
    "\n",
    "# Process each image\n",
    "for idx, row in tqdm(df_valid_frontal.iterrows(), total=len(df_valid_frontal)):\n",
    "    try:\n",
    "        # Load image\n",
    "        img_path = chexpert_root / row['Path']\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "            \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Prepare inputs\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        inputs['prompt_inputs'] = cls_prompts\n",
    "        \n",
    "        # Move to GPU\n",
    "        for key in inputs:\n",
    "            if isinstance(inputs[key], torch.Tensor):\n",
    "                inputs[key] = inputs[key].cuda()\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            output = clf(**inputs)\n",
    "        \n",
    "        # Store predictions (probabilities for each class)\n",
    "        predictions = output['logits'].cpu().numpy()[0]  # Shape: [num_classes]\n",
    "        all_predictions.append(predictions)\n",
    "        \n",
    "        # Get ground truth labels for CheXpert classes\n",
    "        labels = []\n",
    "        for cls in chexpert_classes:\n",
    "            label_val = row[cls]\n",
    "            # Convert: 1.0 -> positive, 0.0 -> negative, NaN/-1.0 -> uncertain (treat as negative)\n",
    "            if pd.isna(label_val) or label_val == -1.0:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(int(label_val))\n",
    "        all_labels.append(labels)\n",
    "        valid_samples += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['Path']}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nSuccessfully processed {valid_samples} samples\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)  # Shape: [num_samples, num_classes]\n",
    "all_labels = np.array(all_labels)  # Shape: [num_samples, num_classes]\n",
    "\n",
    "print(f\"Predictions shape: {all_predictions.shape}\")\n",
    "print(f\"Labels shape: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f74094",
   "metadata": {},
   "source": [
    "### Calculate evaluation metrics (AUC-ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate AUC-ROC for each class\n",
    "auc_scores = []\n",
    "ap_scores = []  # Average Precision\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Zero-Shot Classification Results on CheXpert Validation Set\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nNumber of samples: {valid_samples}\")\n",
    "print(f\"Number of classes: {len(chexpert_classes)}\\n\")\n",
    "\n",
    "for i, cls_name in enumerate(chexpert_classes):\n",
    "    y_true = all_labels[:, i]\n",
    "    y_scores = all_predictions[:, i]\n",
    "    \n",
    "    # Only calculate if we have both positive and negative samples\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        auc = roc_auc_score(y_true, y_scores)\n",
    "        ap = average_precision_score(y_true, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "        ap_scores.append(ap)\n",
    "        \n",
    "        # Calculate accuracy with threshold 0.5\n",
    "        y_pred = (y_scores > 0.5).astype(int)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        print(f\"{cls_name:30s} | AUC: {auc:.4f} | AP: {ap:.4f} | Acc: {acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"{cls_name:30s} | Skipped (only one class present)\")\n",
    "        auc_scores.append(np.nan)\n",
    "        ap_scores.append(np.nan)\n",
    "\n",
    "# Calculate mean scores (excluding NaN)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "mean_ap = np.nanmean(ap_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Mean AUC-ROC:  {mean_auc:.4f}\")\n",
    "print(f\"Mean AP:       {mean_ap:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f195638",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot AUC scores for each class\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "valid_indices = ~np.isnan(auc_scores)\n",
    "valid_classes = [cls for cls, valid in zip(chexpert_classes, valid_indices) if valid]\n",
    "valid_auc_scores = [score for score, valid in zip(auc_scores, valid_indices) if valid]\n",
    "\n",
    "bars = ax.barh(range(len(valid_classes)), valid_auc_scores, color='steelblue')\n",
    "ax.set_yticks(range(len(valid_classes)))\n",
    "ax.set_yticklabels(valid_classes, fontsize=10)\n",
    "ax.set_xlabel('AUC-ROC Score', fontsize=12)\n",
    "ax.set_title('Zero-Shot Classification Performance on CheXpert Validation Set', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=mean_auc, color='red', linestyle='--', linewidth=2, label=f'Mean AUC: {mean_auc:.4f}')\n",
    "ax.legend()\n",
    "ax.set_xlim([0, 1])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, score) in enumerate(zip(bars, valid_auc_scores)):\n",
    "    ax.text(score + 0.01, i, f'{score:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71494fd",
   "metadata": {},
   "source": [
    "### Show sample predictions with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample predictions\n",
    "num_samples_to_show = 4\n",
    "sample_indices = np.random.choice(valid_samples, min(num_samples_to_show, valid_samples), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get image path\n",
    "    row = df_valid_frontal.iloc[sample_idx]\n",
    "    img_path = chexpert_root / row['Path']\n",
    "    \n",
    "    # Load and display image\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Get predictions and ground truth for this sample\n",
    "    predictions = all_predictions[sample_idx]\n",
    "    labels = all_labels[sample_idx]\n",
    "    \n",
    "    # Find top predicted classes and ground truth positives\n",
    "    top_indices = np.argsort(predictions)[-3:][::-1]  # Top 3 predictions\n",
    "    positive_indices = np.where(labels == 1)[0]  # Ground truth positives\n",
    "    \n",
    "    # Build title text\n",
    "    title_text = f\"Sample {sample_idx + 1}\\n\\n\"\n",
    "    title_text += \"Top 3 Predictions:\\n\"\n",
    "    for i in top_indices:\n",
    "        title_text += f\"  • {chexpert_classes[i]}: {predictions[i]:.3f}\\n\"\n",
    "    \n",
    "    if len(positive_indices) > 0:\n",
    "        title_text += \"\\nGround Truth (Positive):\\n\"\n",
    "        for i in positive_indices:\n",
    "            title_text += f\"  • {chexpert_classes[i]} (score: {predictions[i]:.3f})\\n\"\n",
    "    else:\n",
    "        title_text += \"\\nGround Truth: No findings\"\n",
    "    \n",
    "    ax.set_title(title_text, fontsize=9, ha='left', loc='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 789895,
     "sourceId": 1403676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1302315,
     "sourceId": 2169393,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6903464,
     "sourceId": 11076698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8507632,
     "sourceId": 13405643,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3888818,
     "sourceId": 6755629,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 516716,
     "sourceId": 951996,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2609.874984,
   "end_time": "2025-10-17T16:39:52.076906",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-17T15:56:22.201922",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
